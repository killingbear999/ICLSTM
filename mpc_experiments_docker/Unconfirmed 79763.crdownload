#!/usr/bin/python


from __future__ import print_function
import pandas
import keras
from keras.models import Sequential
from keras.layers import Dense, BatchNormalization
from keras.callbacks import EarlyStopping
from scipy.optimize import NonlinearConstraint, LinearConstraint
from scipy.optimize import BFGS, minimize, Bounds, SR1
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, SimpleRNN
import numpy
from keras.models import model_from_json, load_model
from pathlib import Path
import os.path
import time
import os
import math
import pyipopt
from numpy import *
#####Simulation time step
delta=0.01   # sampling time
hc=1e-4     # integration time step
oper_time=0.01  

####Initial states
CAi=-1.65
Ti=72
x1_nn=CAi
x2_nn=Ti
x1_record=[CAi]
x2_record=[Ti]
u1_record=[]
u2_record=[]


a=1060
b=22
d=0.52  # Lyapunov function V=x^T*P*x


# CSTR PARAMETERS
F=5
V=1
k0=8460000
E=50000
R=8.314
T0=300
Dh=-11500
rho=1000
sigma=1000
Cp=0.231
cp=0.231
Qs=0
CA0s=4

#steady-state
CAs= 1.9537
Ts=  401.8727
state_ss=numpy.array([Ts, CAs])
input_ss=numpy.array([Qs, CA0s])




ROOT_FOLDER=os.getcwd()
#### CONSTANTS ####
NUM_MPC_ITERATION=10   # MPC TOTAL ITERATION

NUM_MODELS=1  # USELESS
NUM_SUBMODELS=1  # USELESS
NUM_OUTPUTS=2  
NUM_INPUTS=2 ##4   #used to be 16   ??
HORIZON=2   ## MPC PREDICTION HORIZON

#NUM_CONSTRAINT_MODELS=3
NUM_IN_SEQUENCE=int(0.01/(hc*10))  ## USELESS
PREDICTION_STORE=0   ## 
#NUM_PREDICTION_STEPS=1
deviation=0
NUM_MPC_INPUTS=NUM_INPUTS*HORIZON  ##
NUM_MPC_CONSTRAINTS=HORIZON
realtime_data=None
setpoint=[0, 0]

 

def my_ens_prediction(num_horizon,my_rawdata,my_inputs): #my_rawdata: current CA, T; my_inputs: u1, u2 to be optimzed
    xx = []
    nn_inputs = []
    NUM_INTERMEDIATE_STEPS=NUM_IN_SEQUENCE
    ensemble_output = numpy.zeros((num_horizon,NUM_OUTPUTS,NUM_INTERMEDIATE_STEPS))
    #numpy.array([0]*(num_horizon*NUM_OUTPUTS*NUM_IN_SEQUENCE))#[[[0 for i in range(NUM_IN_SEQUENCE)] for j in range(NUM_OUTPUTS)] for j in range(NUM_IN_SEQUENCE)]
    ensemble_output = ensemble_output.reshape(num_horizon,NUM_INTERMEDIATE_STEPS,NUM_OUTPUTS)
    predict_output = []
    x_test2 = my_rawdata[0:NUM_OUTPUTS].astype(float)  ## CA, T
    #x_test2= (x_test2+state_ss-state_mean)/state_std

    predict_output_normal=[[0 for i in range(NUM_OUTPUTS)] for j in range(NUM_IN_SEQUENCE)] ## INITALIZATION
    x1=x_test2[1]  ## CA
    x2=x_test2[0]  ## T
    for i_model in range(num_horizon):  ## Explicit euler  
    ##  my_inputs[0]: u1(t=0), my_inputs[1]: u2(t=0) ; my_inputs[2]=u1(t=1), my_inputs[3]=u1(t=1)
        for kk in range (int(delta/hc)):
        #print ("Anh needs to print x1 and x2: ", x1, x2)
        #x1_new = x1 + hc * ((F / V) * (u1  - x1) - k0 * (numpy.exp(-E / (R * x2))*x1 * x1))
        #x2_new = x2 + hc * ((F / V) * (T0-x2) + (-Dh / (sigma * cp)) * k0 * (numpy.exp(-E / (R * x2)) * x1* x1)
        #        + (u2 / (sigma * cp * V)))
            x1_new = x1 + hc * ((F / V) * (my_inputs[2*i_model+1] - x1) -
                            k0 * ((numpy.exp(-E / (R * (x2 + Ts)))*(x1 + CAs) * (x1 + CAs))
                                  - numpy.exp(-E / (R * Ts)) * CAs * CAs))

            x2_new = x2 + hc * (((F / V) * (-x2) + (-Dh / (sigma * cp)) *
                             (k0 * ((numpy.exp(-E / (R * (x2 + Ts))) * (x1 + CAs) * (x1 + CAs)) -
                                      numpy.exp(-E / (R * Ts)) * CAs * CAs)) + (my_inputs[2*i_model] / (sigma * cp * V))))



            x1 = x1_new
            x2 = x2_new
            if (kk % 10 == 0):
                predict_output = [x2, x1]
                ensemble_output[i_model, int(kk / 10), 0: 2] = predict_output

        #predict_output=[x2, x1]
        #ensemble_output[i_model, :,0: 2] = predict_output

    # for i_model in range(num_horizon):
    #     my_inputs_normalized = (my_inputs[2*i_model:2*(i_model+1)]+ input_ss - input_mean) / input_std
    #     sum=[[0 for i in range(NUM_OUTPUTS)] for j in range(NUM_IN_SEQUENCE)]
    #     xx = numpy.concatenate((x_test2,  my_inputs_normalized), axis=None).reshape((1, NUM_INPUTS))
    #     xx = numpy.tile(xx, (NUM_IN_SEQUENCE, 1))
    #
    #     nn_inputs = xx.reshape(1, NUM_IN_SEQUENCE, NUM_INPUTS)
    #     for j_submodel in range (NUM_SUBMODELS):
    #         # j_submodel=0
    #         predict_output = numpy.array(model[j_submodel].predict(nn_inputs))
    #         predict_output = predict_output.reshape(NUM_IN_SEQUENCE, NUM_OUTPUTS)
    #         # print ("submodel id:", j_submodel)
    #
    #         sum=sum+predict_output
    #         #COUNT_CORRECT_MODEL=COUNT_CORRECT_MODEL+1
    #     # MODEL AVERAGING (ENSEMBLE LEARNING)
    #     predict_output=sum/NUM_SUBMODELS
    #     x_test2=predict_output[-1,0:2]
    #
    #     # RESCALING BY THE CORRESPONDING STANDARD DEVIATION & THE MEAN OF THE OUTPUT STATISTICS OF THE EXITING SURFACE
    #     predict_output_normal = predict_output * output_std + output_mean
    #     ensemble_output[i_model,:,:]=predict_output_normal

    return ensemble_output    



#################################################
################## MPC PROGRAM ##################
#################################################
### DEFINE THE UPPER BOUND AND LOWER BOUND OF THE MANIPULATED INPUTS ###

def eval_f(x):    ## objective function
    assert len(x) == int(NUM_MPC_INPUTS)
    offset=0
    global PREDICTION_STORE
    #### CALCULATE OUTLET CONC ###########
    df_ensemble_output = my_ens_prediction(num_horizon=int(NUM_MPC_INPUTS/2),my_rawdata=realtime_data,my_inputs=x)
    # LAST SUBENSEMBLE, LAST TIME STEP, FIRST VARIABLE
    factor = realtime_data[1] ** 2 * 500 / (realtime_data[0] ** 2 * 50)
    factor2 = realtime_data[1] ** 2 * 500 / (3.5 ** 2 * 10000)
    #### account for all intermediate steps ####
    for j in range (int(NUM_MPC_INPUTS/2)):
        est_outlet_product = df_ensemble_output[j, :, 0:2]
        for i in range (NUM_IN_SEQUENCE):
            #offset = (setpoint[0]-(est_outlet_product[i,0])) ** 2.0 *2 +(setpoint[1]-(est_outlet_product[i,1])) ** 2.0 *500 +\
             #    x[2*j] **2 *5e-6 + 0.6* x[2*j+1] **2
             offset = offset+(setpoint[0]-(est_outlet_product[i,0])) ** 2.0 *0.5 +(setpoint[1]-(est_outlet_product[i,1])) ** 2.0 *500 +\
                  0#x[2*j] **2 *1e-6 + 0.6* x[2*j+1] **2
        #offset = offset + x[2 * j] ** 2 * factor2 * 3e-10 + factor2 * x[2 * j + 1] ** 2


   ## offset= offset + (T_pred-T_ss)^2 


    #print ("im here")
    #####  only account for the last point #####
    # for j in range(int(NUM_MPC_INPUTS / 2)):
    #     est_outlet_product = df_ensemble_output[j, -1, 0:2]
    #     #for i in range(NUM_IN_SEQUENCE):
    #     offset = offset+(setpoint[0] - (est_outlet_product[0])) ** 2.0*0.5+ \
    #              (setpoint[1] - (est_outlet_product[1])) ** 2.0 * 500 + \
    #              x[2*j] ** 2 * 1e-10 + 0.05 * x[2*j+1] ** 2

    # useless for now
    #PREDICTION_STORE=numpy.asscalar(est_outlet_product)
    #print("Prediction store:", PREDICTION_STORE)
    #print ("obj func:  ", numpy.asarray(offset))
    return offset/100

def eval_grad_f(x):  ## gradient of objective function
    assert len(x) == int(NUM_MPC_INPUTS)
    step = 1.e-3 # we just have a small step
    objp=objm=0
    grad_f = [0]*NUM_MPC_INPUTS
    xpstep = [0]*NUM_MPC_INPUTS
    xmstep = [0]*NUM_MPC_INPUTS
    for i_mpc_input in range(NUM_MPC_INPUTS):
        xpstep=x.copy()
        xmstep=x.copy()
        # for each variables, we need to evaluate the derivative of the function with respect to that variable, This is why we have the for loop
        xpstep[i_mpc_input]  = xpstep[i_mpc_input]+step 
        xmstep[i_mpc_input] = xmstep[i_mpc_input]-step
        #print ("step: ", step)
        #print ("xp:  ",xpstep)
        #print ("xm:  ",xmstep)
        #print ("i_mpc_input:  ",i_mpc_input)
        # Evaluate the objective function at xpstep and xmstep
        objp=eval_f(xpstep) # This function returns the value of the objective function evaluated with the variable x[i] is perturebed +step
        objm=eval_f(xmstep) # This function returns the value of the objective function evaluated with the variable x[i] is perturebed -step
        #print ("obj ", objp, "   objm   ", objm)
        grad_f[i_mpc_input] = (objp - objm) / (2 * step) # This evaluates the gradient of the objetive function with repect to the optimization variable x[i]
    #print("Gradient: ", grad_f)
    return array(grad_f)

def eval_g(x):  ## MPC constraints
    assert len(x) == int(NUM_MPC_INPUTS)
    #### CALCULATE FLUID TEMPERATURE ALONG THE FIRST THREE SURFACES ###########
    #df_ensemble_output = my_ens_prediction(num_horizon=NUM_CONSTRAINT_MODELS,my_rawdata=realtime_data,my_inputs=x)
    CAd2=realtime_data[1]  ## current CA
    Td2=realtime_data[0]  ## current T
    ##pred_states=df_ensemble_output[-1, -1, 0:2]
    #CAd2=df_ensemble_output[-1, -1, 1]
    #Td2
    g=array([-5.0]*NUM_MPC_CONSTRAINTS)
    #print ("gggg", g)

    if ((a*CAd2**2+d*Td2**2+2*b*CAd2*Td2-2)> 0):  ## If V > \rho_min=2
        LfV = (2 * a * CAd2 + 2 * b * Td2) * ((F / V) * (-CAd2) - k0 * ((numpy.exp(-E / (R * (Td2 + Ts))) * (CAd2 + CAs) ** 2)-
                                                                        numpy.exp(-E / (R * Ts)) *(CAs)**2)) + \
              (2 * d * Td2 + 2 * b * CAd2) * (((F / V) * (-Td2) + (-Dh / (sigma * cp)) *
                                               (k0 * ((numpy.exp(-E / (R * (Td2 + Ts))) *(CAd2 + CAs) ** 2) - numpy.exp(-E / (R * Ts)) * CAs ** 2))))

        #LfV= dV/dx * f

        LgV = (2 * d * Td2 + 2 * b * CAd2) / (V * sigma * cp)  #LgV= dV/dx * g
 
        h2x = -(LfV + sqrt((LfV ** 2) + LgV ** 4)) / (LgV)  # h2x= \phi_nn(x)


        if (h2x > 5e5):
            h2x=5e5

        if (h2x < -5e5):
            h2x=-5e5

        dot_Vt = (2 * a * CAd2 + 2 * b * Td2) * ((F / V) * (0 - CAd2) -
                                                 k0 * ((numpy.exp(-E / (R * (Td2 + Ts))) * (
                            CAd2 + CAs) ** 2) - numpy.exp(
                    -E / (R * Ts)) * (CAs ** 2))) + \
                 (2 * d * Td2 + 2 * b * CAd2) * (((F / V) * (-Td2) + (-Dh / (sigma * cp)) * (
                k0 * ((numpy.exp(-E / (R * (Td2 + Ts))) * (CAd2 + CAs) ** 2) -
                      numpy.exp(-E / (R * Ts)) * CAs ** 2)) + (h2x / (sigma * cp * V))))

                 #dot_Vt= \dot V on RHS, \dot V= dV/dx * dx/dt

        dot_V=(2*a * CAd2 + 2*b * Td2)*((F / V)*(x[1] - CAd2) -
               k0*((numpy.exp(-E / (R*(Td2 + Ts)))* (CAd2 + CAs) ** 2) - numpy.exp(-E / (R*Ts))*(CAs ** 2))) + \
            (2*d*Td2 + 2*b * CAd2)*(((F / V)*(-Td2) + (-Dh / (sigma*cp))*(k0*((numpy.exp(-E / (R*(Td2 + Ts)))* (CAd2 + CAs)** 2) -
              numpy.exp(-E / (R*Ts))* CAs ** 2)) + (x[0] / (sigma*cp*V))))

            ## dot_V= \dot V on LHS
        #print ("dot V", dot_V)
        g[0]=dot_V-dot_Vt#+1#e-2
        #g[0]=dot_V+10

    else:
        ### if V > \rho_min=2
        df_ensemble_output2 = my_ens_prediction(num_horizon=int(NUM_MPC_INPUTS / 2), my_rawdata=realtime_data,
                                               my_inputs=x)
        # LAST SUBENSEMBLE, LAST TIME STEP, FIRST VARIABLE

        #### NORMALIZE SETPOINT ####
        # for j in range (int(NUM_MPC_INPUTS/2)):
        #     est_outlet_product = df_ensemble_output[j, :, 0:2]
        #     for i in range (NUM_IN_SEQUENCE):
        #         offset = (setpoint[0]-(est_outlet_product[i,0]-Ts)) ** 2.0 +(setpoint[1]-(est_outlet_product[i,1]-CAs)) ** 2.0 *500 +\
        #              x[0] **2 *1e-11 + 0.05* x[1] **2

        #####  only account for the last point #####
        for j in range(int(NUM_MPC_INPUTS / 2)):
            est_outlet_product2 = df_ensemble_output2[j, -1, 0:2]
            # for i in range(NUM_IN_SEQUENCE):
            g[j]= a * est_outlet_product2[1] ** 2+ 2 * b * est_outlet_product2[1]*est_outlet_product2[0] + \
                  d*est_outlet_product2[0] ** 2 -1.8  # V=a*CA^2+ d*T^2 +2*b*CA*T
  

    #df_fluid_temp=df_fluid_temp.reshape(NUM_CONSTRAINT_MODELS*NUM_IN_SEQUENCE)
    #print ("constraints satisfaction:  ",dot_V)
    return  g

nnzj = NUM_MPC_CONSTRAINTS*NUM_MPC_INPUTS


def eval_jac_g(x, flag):  ## JOCABIAN OF constraints
    #print ("in eval_jac_g_0")
    if flag:
        list_x = []
        list_y=[]
        for i in range(int(NUM_MPC_INPUTS / 2)):
            list_x = list_x + [i] * NUM_MPC_INPUTS
            list_y = list_y +list(range(0, int(NUM_MPC_INPUTS)))
        #list_x=[0]*int(NUM_MPC_INPUTS)+[1]*int(NUM_MPC_INPUTS)
        #list_y=list(range(0, int(NUM_MPC_INPUTS)))+list(range(0, int(NUM_MPC_INPUTS)))
        #print ("list_x:", list_x)
        #print("list_y:", list_y)
        return (array(list_x),
                array(list_y))

        #return (array([0, 0]),
        #        array([0, 1]))
        #print ("in eval_jac_g_1")
    else:
        assert len(x) == int(NUM_MPC_INPUTS)
        step = 1e-3 # we just have a small step
        gp=gm=numpy.zeros(NUM_MPC_CONSTRAINTS)
        xpstep=xmstep=numpy.zeros(NUM_MPC_INPUTS)
        jac_g = [[0]*int(NUM_MPC_INPUTS) for _ in range(NUM_MPC_CONSTRAINTS)]
        #print ("shape:", jac_g)
        for i_mpc_input in range(NUM_MPC_INPUTS):
            xpstep=x.copy()
            xmstep=x.copy()
            # for each variables, we need to evaluate the derivative of the function with respect to that variable, This is why we have the for loop
            xpstep[i_mpc_input] += step 
            xmstep[i_mpc_input] -= step
            gp=eval_g(xpstep)
            gm=eval_g(xmstep)
            for num_constraint in range(NUM_MPC_CONSTRAINTS):
                jac_g[num_constraint][i_mpc_input] = (gp[num_constraint] - gm[num_constraint]) / (2 * step)
            #print ("in eval_jac_g_2:")
        return array(jac_g)

def apply_new(x):
    return True
def print_variable(variable_name, value):
    for i in range(len(value)):
        print("{} {}".format(variable_name + "["+str(i)+"] =", value[i]))


nnzh = NUM_MPC_INPUTS**2  ## non-zero elements in Hessian matrix

# def eval_h(x, lagrange, obj_factor, flag, user_data = None):
#     if flag:
#         hrow = [0, 1, 1, 2, 2, 2, 3, 3, 3, 3]
#         hcol = [0, 0, 1, 0, 1, 2, 0, 1, 2, 3]
#         return (array(hcol), array(hrow))
#     else:
#         values = zeros((10), float_)
#         values[0] = obj_factor * (2*x[3])
#         values[1] = obj_factor * (x[3])
#         values[2] = 0
#         values[3] = obj_factor * (x[3])
#         values[4] = 0
#         values[5] = 0
#         values[6] = obj_factor * (2*x[0] + x[1] + x[2])
#         values[7] = obj_factor * (x[0])
#         values[8] = obj_factor * (x[0])
#         values[9] = 0
#         values[1] += lagrange[0] * (x[2] * x[3])

#         values[3] += lagrange[0] * (x[1] * x[3])
#         values[4] += lagrange[0] * (x[0] * x[3])

#         values[6] += lagrange[0] * (x[1] * x[2])
#         values[7] += lagrange[0] * (x[0] * x[2])
#         values[8] += lagrange[0] * (x[0] * x[1])
#         values[0] += lagrange[1] * 2
#         values[2] += lagrange[1] * 2
#         values[5] += lagrange[1] * 2
#         values[9] += lagrange[1] * 2
#         return values

#####################################################################
##### PRE-PROCESSING (THE FOLLOWING COMMANDS ARE EXECUTED ONCE) #####
#####################################################################
#### LOAD MEAN AND STD FILES###########
#### READ MEANS & STD FROM THE FILE #####
    
    
####################################################################
##### SOLVING THE MPC PROGRAM TO FIND THE OPTIMIZED MPC INPUTS #####
####################################################################
##########  KEEP RUNNING MPC ###############

dir_name = os.getcwd()
test = os.listdir(dir_name)

for item in test:
    if item.endswith(".txt"):
        os.remove(os.path.join(dir_name, item))


nvar = NUM_MPC_INPUTS

##  DEFINE THE UPPER BOUND AND LOWER BOUND FOR optimized variables ###
x_lower=[0]* nvar
x_upper=[0]* nvar
for i in range(int(NUM_MPC_INPUTS/2)):
    x_lower[2*i]=-5e5
    x_lower[2 * i+1] = -3.5
    x_upper[2 * i] = 5e5
    x_upper[2 * i + 1] = 3.5
x_L = array(x_lower) #array([-5e5, -3.5])
x_U = array(x_upper) #array([5e5, 3.5])

### DEFINE THE UPPER BOUND AND LOWER BOUND OF THE CONSTRAINT ###
ncon = NUM_MPC_CONSTRAINTS
g_L = array([-2e19]*HORIZON)
g_U = array([0]*HORIZON)

print ("g_L", g_L, g_U)


for main_iteration in range(NUM_MPC_ITERATION):
    print ("Num Iteratin: ", main_iteration)
    #check_file = Path("indicator.out")
    #while not os.path.exists(check_file):
    #    time.sleep(1)
    #os.remove(check_file)  

    #if main_iteration >0:
     #   deviation=deviation-PREDICTION_STORE
     #   setpoint = SETPOINT_TRUE-deviation
    #else:
    #    setpoint = SETPOINT_TRUE 
    #print ("setpoint: ", setpoint)
    #print ("deviation: ", deviation)
    #print ("PREDICTION_STORE: ",PREDICTION_STORE)
    

    rawdata=numpy.array([Ti, CAi])  ## global variable

    #### NORMALIZE RAW DATA ####
    #rawdata=(rawdata-state_mean)/state_std
    # print ("normalized data:  ", rawdata)
    realtime_data=rawdata   ## global variable

    nlp = pyipopt.create(nvar, x_L, x_U, ncon, g_L, g_U, nnzj, nnzh, eval_f, eval_grad_f, eval_g, eval_jac_g)
    #x0 = array([3.33, 3.8, 3.8, 6])


    ### INITIAL GUESS of X ###
    if main_iteration ==0 :
        x0 = array([0.0]*int(NUM_MPC_INPUTS))
    else:
        x0=x
        x0[0:-2]=x[2:]
        x0[-2:]=[0, 0]
        #x0=array(x[:-2], 0, 0)
    """
    print x0
    print nvar, ncon, nnzj
    print x_L,  x_U
    print g_L, g_U
    print eval_f(x0)
    print eval_grad_f(x0)
    print eval_g(x0)
    a =  eval_jac_g(x0, True)
    print "a = ", a[1], a[0]
    print eval_jac_g(x0, False)
    print eval_h(x0, pi0, 1.0, False)
    print eval_h(x0, pi0, 1.0, True)
    """

    """ You can set Ipopt options by calling nlp.num_option, nlp.str_option
    or nlp.int_option. For instance, to set the tolarance by calling

        nlp.num_option('tol', 1e-8)

    For a complete list of Ipopt options, refer to

        http://www.coin-or.org/Ipopt/documentation/node59.html

    Note that Ipopt distinguishs between Int, Num, and Str options, yet sometimes
    does not explicitly tell you which option is which.  If you are not sure about
    the option's type, just try it in PyIpopt.  If you try to set one type of
    option using the wrong function, Pyipopt will remind you of it. """
    nlp.int_option('max_iter', 500)
    nlp.num_option('tol', 1e-5)
    nlp.int_option('print_level', 2)
    print("Going to call solve")
    print("x0 = {}".format(x0))
    x, zl, zu, constraint_multipliers, obj, status = nlp.solve(x0)



    # import pdb; pdb.set_trace()
    nlp.close()

    print("Solution of the primal variables, x")
    print_variable("x", x)

    # print("Solution of the bound multipliers, z_L and z_U")
    # print_variable("z_L", zl)
    # print_variable("z_U", zu)

    # print("Solution of the constraint multipliers, lambda")
    # print_variable("lambda", constraint_multipliers)

    print("Objective value")
    print("f(x*) = {}".format(obj))
    print ("Control action=:  ", x[1], x[0])
    #REAL_CONTROL_ACTION=x*
    #print ("REAL DATA:", REAL_CONTROL_ACTION)
    #numpy.savetxt("input_to_fluent.out",   REAL_CONTROL_ACTION, fmt="%f",  delimiter=" ")
    
    ## Apply optimal control actions u1 and u2 to real system

    ## apply only the first control action

    x1=CAi
    x2=Ti

    for kk in range (int(delta/hc)):
        #print ("Anh needs to print x1 and x2: ", x1, x2)
        #x1_new = x1 + hc * ((F / V) * (u1  - x1) - k0 * (numpy.exp(-E / (R * x2))*x1 * x1))
        #x2_new = x2 + hc * ((F / V) * (T0-x2) + (-Dh / (sigma * cp)) * k0 * (numpy.exp(-E / (R * x2)) * x1* x1)
        #        + (u2 / (sigma * cp * V)))


        x1_new = x1 + hc * ((F / V) * (x[1] - x1) -
                            k0 * ((numpy.exp(-E / (R * (x2 + Ts)))*(x1 + CAs) * (x1 + CAs))
                                  - numpy.exp(-E / (R * Ts)) * CAs * CAs))

        x2_new = x2 + hc * (((F / V) * (-x2) + (-Dh / (sigma * cp)) *
                             (k0 * ((numpy.exp(-E / (R * (x2 + Ts))) * (x1 + CAs) * (x1 + CAs)) -
                                      numpy.exp(-E / (R * Ts)) * CAs * CAs)) + (x[0] / (sigma * cp * V))))



        x1 = x1_new;
        x2 = x2_new;

    CAi=x1
    Ti=x2

        #print ("First principle intermediate steps x1, x2:  ", x1, x2)
        #print("First principle intermediate derivatives x1, x2:  ", x1_derivate, x2_derivate)

    #x1=x1-CAs
    #x2=x2-Ts
    print('Real model output x1 x2 in deviation form:   ', x1, x2)
    #x1=(x1 -y1_mean)/y1_std
    #x2 = (x2 - y2_mean) / y2_std
    x1_record.append(x1)
    x2_record.append(x2)
    u1_record.append(x[1])
    u2_record.append(x[0])

print ("x1_record: ",x1_record)
print ("x2_record: ",x2_record)

print ("u1_record: ",u1_record)
print ("u2_record: ",u2_record)



#
numpy.savetxt("x1.txt",   x1_record, fmt="%f",  delimiter=" ")
numpy.savetxt("x2.txt",   x2_record, fmt="%f",  delimiter=" ")


numpy.savetxt("u1.txt",   u1_record, fmt="%f",  delimiter=" ")
numpy.savetxt("u2.txt",   u2_record, fmt="%f",  delimiter=" ")




